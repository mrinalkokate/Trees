# -*- coding: utf-8 -*-
"""DecisionTree2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vTqnUKuTyztm5MOGe-JzEMSC18obyRZb
"""

# Data Preparation
from pandas import read_csv, get_dummies,Series,DataFrame
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
data=read_csv('/content/drive/MyDrive/Colab Notebooks/Emloyees.csv')# reading
data['PastEmployee'] = data['PastEmployee'].map({'Yes':0, 'No':1}) # encoding output
data['OverTime'] = data['OverTime'].map({'Yes':1, 'No':0}) # encoding
data['Gender'] = data['Gender'].map({'Female':1, 'Male':0}) # encoding
data2 = get_dummies(data, columns =  ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus']) # encoding
X = data2.drop('PastEmployee', axis = 1) # Features
Y = data2['PastEmployee'] # Labels
X_scaled = StandardScaler().fit_transform(X) # scaling
X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size = 0.2, random_state = 100)# splitting
X_train,Y_train =SMOTE (random_state = 100).fit_resample(X_train,Y_train)# balancing

# Decision Tree Classifier (method#1)
from sklearn import tree
#from sklearn.tree import DecisionTreeClassifier
# import sklearn as  sk
# import sklearn
DT_classifier =tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 5)# classifier building
DT_classifier.fit(X_train, Y_train) # training
Y_pred = DT_classifier.predict(X_test) # testing
# imp_features =Series(DT_classifier.feature_importances_, index=list(X)).sort_values(ascending=False) # what are the important features?
# print(imp_features)

# Evaluation
# Acuracy and confusion matrix
from sklearn import metrics
Accuracy=metrics.accuracy_score(Y_test, Y_pred) # calculating accuaracy
print("Accuracy: ", Accuracy) # Is this a good metric??
con_matrix = metrics.confusion_matrix(Y_test, Y_pred)
print (con_matrix)
recall = metrics.recall_score(Y_test, Y_pred)
print ("recall :",recall)
percision=metrics.precision_score(Y_test, Y_pred)
print("percesion :",percision)

#using GridSearch (method#2)
from sklearn.model_selection import GridSearchCV
DT_classifier2 = tree.DecisionTreeClassifier(criterion = 'entropy') # building classfier
depth = {'max_depth': [2,3,4,5,10,15,20,25,30,35]}
grid_search1 = GridSearchCV(estimator=DT_classifier2, param_grid=depth, scoring='precision', cv=5)# building
grid_search1.fit(X_scaled, Y )# training, testing , evaluation, ranking.
best_depth = grid_search1.best_params_
print(best_depth)
best_result = grid_search1.best_score_
print(best_result)

# Decision Tree Classifier (method#1) with th best depth
from sklearn import tree
DT_classifier3 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 15)# classifier building
DT_classifier3.fit(X_train, Y_train) # training
Y_pred2 = DT_classifier3.predict(X_test) # testing
imp_features = Series(DT_classifier3.feature_importances_, index=list(X)).sort_values(ascending=False) # what are the important features?
print(imp_features)

#using Grid Search (method#2) with best feature
X2 = data2[['OverTime','MonthlyIncome','MaritalStatus_Single','EnvironmentSatisfaction','Age']] # Features#
#Y = data2['PastEmployee'] # Labels
X_scaled = StandardScaler().fit_transform(X2) # scaling
# X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size = 0.3, random_state = 100)# splitting
# X_train,Y_train =SMOTE (random_state = 100).fit_resample(X_train,Y_train)# balancing

DT_classifier4 = tree.DecisionTreeClassifier(criterion = 'entropy') # building classfier
depth = {'max_depth': [2,3,4,5,10,15,20,25,30,35]}
grid_search2 = GridSearchCV(estimator=DT_classifier4, param_grid=depth, scoring='recall', cv=5)
grid_search2.fit(X_scaled, Y) # Training, testing , evaluation, ranking.
best_depth = grid_search2.best_params_
print(best_depth)
best_result = grid_search2.best_score_
print(best_result)

# Decision Tree Classifier (method#1) with the best depth and best features
X3 = data2[['OverTime','MonthlyIncome','MaritalStatus_Single','EnvironmentSatisfaction','Age']] # Features
#Y = data2['PastEmployee'] # Labels
X_scaled = StandardScaler().fit_transform(X3) # scaling
X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size = 0.3, random_state = 100)# splitting
X_train,Y_train =SMOTE (random_state = 100).fit_resample(X_train,Y_train)# balancing
DT_classifier5 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 3)# classifier building
DT_classifier5.fit(X_train, Y_train) # training
Y_pred3 = DT_classifier5.predict(X_test) # testing
# imp_features = Series(DT_classifier5.feature_importances_, index=list(X)).sort_values(ascending=False) # what are the important features?
# print(imp_features)
# Acuracy and confusion matrix
from sklearn import metrics
Accuracy=metrics.accuracy_score(Y_test, Y_pred3) # calculating accuaracy
print("Accuracy: ", Accuracy) # Is this a good metric??
con_matrix = metrics.confusion_matrix(Y_test, Y_pred3)
print (con_matrix)
recall = metrics.recall_score(Y_test, Y_pred3)
print (recall)
percision=metrics.precision_score(Y_test, Y_pred3)
print(percision)